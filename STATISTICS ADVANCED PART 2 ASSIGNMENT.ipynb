{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "square of chi is  0.0\n",
      "value of p is 1.0\n",
      "fail to reject the null hypothesis\n"
     ]
    }
   ],
   "source": [
    "#1- What is hypothesis testing in statistics ?\n",
    "\n",
    "#   Hypothesis testing in statistics is a method used to determine whether there is enough evidence to support a \n",
    "#   specific claim or hypothesis about a population based on sample data. Essentially, it helps you assess whether\n",
    "#   the observed data fits a particular hypothesis or if an alternative hypothesis is more likely.\n",
    "\n",
    "\n",
    "\n",
    "#2- What is the null hypothesis, and how does it differ from the alternative hypothesis ?\n",
    "\n",
    "#   The null hypothesis (H₀) and the alternative hypothesis (H₁ or Ha) are two key concepts in hypothesis testing,\n",
    "#   and they represent opposing statements about a population parameter.\n",
    "\n",
    "#   Null Hypothesis (H₀)----\n",
    "#   The null hypothesis is the default assumption that there is no effect, no difference, or no relationship in the \n",
    "#   population. It's often a statement of \"no change\" or \"no association.\"\n",
    "#   It serves as the baseline that you test against to see if your sample data provides enough evidence to reject it.\n",
    "\n",
    "#   Alternative Hypothesis (H₁ or Ha)----\n",
    "#   The alternative hypothesis represents a statement that contradicts the null hypothesis. It suggests that there \n",
    "#   is an effect, a difference, or a relationship that exists in the population.\n",
    "#   It’s what you’re typically trying to support or prove through your data analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#3- What is the significance level in hypothesis testing, and why is it important ?\n",
    "\n",
    "#   The significance level (α) in hypothesis testing is a threshold that determines the probability of rejecting \n",
    "#   the null hypothesis when it is actually true (i.e., committing a Type I error). In simpler terms, it's the \n",
    "#   level of risk you're willing to accept for making a false positive conclusion — that is, claiming there's an \n",
    "#   effect or difference when there actually isn't.\n",
    "\n",
    "#   Why is the significance level important?\n",
    "#1. Balancing Errors >>>>\n",
    "\n",
    "#   Type I Error (False Positive): Rejecting the null hypothesis when it is actually true.\n",
    "#   Type II Error (False Negative): Failing to reject the null hypothesis when it is actually false.\n",
    "#   By setting an appropriate significance level, you're deciding how much risk you're willing to take for making \n",
    "#   a Type I error. If you're stricter (e.g., setting α to 0.01), you're reducing the chance of a Type I error but\n",
    "#   increasing the risk of a Type II error (failing to detect a true effect).\n",
    "\n",
    "#2. Interpretation of Results >>>>\n",
    "\n",
    "#   The significance level helps you interpret the strength of the evidence against the null hypothesis. A smaller\n",
    "#   α (like 0.01) means you need stronger evidence to reject the null hypothesis. A larger α (like 0.10) means you \n",
    "#   can reject the null hypothesis with weaker evidence.\n",
    "\n",
    "#3. Consistency and Comparisons >>>>\n",
    "\n",
    "#   It ensures consistency in how tests are performed. By using a pre-defined significance level, you can compare \n",
    "#   results across studies and analyses.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#4- What does a P-value represent in hypothesis testing ?\n",
    "\n",
    "#   A p-value in hypothesis testing represents the probability of obtaining a result at least as extreme as the one \n",
    "#   observed, assuming that the null hypothesis is true. In simpler terms, the p-value tells you how likely it is \n",
    "#   that the observed data occurred by random chance, under the assumption that there is no effect or difference \n",
    "#   (i.e., the null hypothesis is true).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#5- How do you interpret the P-value in hypothesis testing ?\n",
    "\n",
    "#   Here’s how you can interpret the p-value step-by-step -----\n",
    "\n",
    "#1. Compare the p-value to the Significance Level (α) >>>\n",
    "#   Before conducting the test, you choose a significance level (usually α = 0.05, but sometimes it can be 0.01\n",
    "#   or 0.10 depending on the context).\n",
    "#   The p-value is then compared to this significance level to determine whether the result is statistically \n",
    "#   significant.\n",
    "\n",
    "#2. Interpreting the P-value >>>\n",
    "#   If p-value ≤ α (e.g., p-value = 0.03 and α = 0.05):\n",
    "\n",
    "#  Reject the null hypothesis (H₀).\n",
    "#  The p-value tells you that there’s a small probability (in this case, 3%) of observing the data (or something \n",
    "#  more extreme) if the null hypothesis is true.\n",
    "#  This suggests that the evidence against the null hypothesis is strong enough to reject it in favor of the \n",
    "#  alternative hypothesis.\n",
    "#  Conclusion: The result is statistically significant.\n",
    "\n",
    "#  If p-value > α (e.g., p-value = 0.08 and α = 0.05):\n",
    "\n",
    "#  Fail to reject the null hypothesis (H₀).\n",
    "#  The p-value tells you that the observed data is fairly likely under the null hypothesis (8% chance of observing \n",
    "#  the data, assuming H₀ is true).\n",
    "#  Since this is higher than your chosen significance level (α), you don't have enough evidence to reject H₀.\n",
    "#  Conclusion: The result is not statistically significant.\n",
    "\n",
    "#3. What does the p-value actually mean?\n",
    "#   The p-value is the probability of observing the data (or something more extreme) under the assumption that\n",
    "#   the null hypothesis is true.\n",
    "#   A smaller p-value means the data is less likely to have occurred if the null hypothesis were true, which \n",
    "#   increases your confidence in rejecting the null hypothesis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#6- What are Type 1 and Type 2 errors in hypothesis testing ?\n",
    "\n",
    "#   In hypothesis testing, Type I and Type II errors are two possible mistakes you can make when making decisions \n",
    "#   about the null hypothesis (H₀). They occur based on whether you reject or fail to reject H₀, and whether H₀ is\n",
    "#   actually true or false in reality.\n",
    "\n",
    "#   Type I Error (False Positive) ---\n",
    "#   A Type I error occurs when you incorrectly reject the null hypothesis (H₀) when it is actually true.\n",
    "#   In simpler terms, you mistakenly conclude that there is an effect, difference, or relationship when there really\n",
    "#   isn’t one.\n",
    "#   The probability of making a Type I error is denoted by α (alpha), which is also called the significance level \n",
    "#   of the test (often set at 0.05).\n",
    "\n",
    "#   Example >>  You are testing a new drug to see if it lowers blood pressure. If you conclude that the drug works \n",
    "#   when it actually doesn’t, you’ve made a Type I error\n",
    "\n",
    "#   Type II Error (False Negative) >>\n",
    "#   A Type II error occurs when you fail to reject the null hypothesis (H₀) when it is actually false.\n",
    "#   In simpler terms, you miss detecting a true effect, difference, or relationship because you conclude there is\n",
    "#   no effect when there actually is one.\n",
    "#   The probability of making a Type II error is denoted by β (beta).\n",
    "#   Example >> You are testing the same drug and conclude that it does not lower blood pressure when it actually \n",
    "#   does, leading you to miss a true beneficial effect.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#7- What is the difference between a one-tailed and a two-tailed test in hypothesis testing ?\n",
    "\n",
    "#   In hypothesis testing, one-tailed and two-tailed tests refer to how the region of rejection (where you would\n",
    "#   reject the null hypothesis) is defined in relation to the distribution of the test statistic. The key difference\n",
    "#   lies in the direction of the test — whether you are looking for an effect in a specific direction (one-tailed) \n",
    "#   or any effect in either direction (two-tailed).\n",
    "\n",
    "#1. Two-Tailed Test >>>\n",
    "#   Definition >>  A two-tailed test is used when you are testing for the possibility of an effect in either \n",
    "#   direction (both positive and negative).\n",
    "\n",
    "#   Null Hypothesis (H₀) >> The null hypothesis typically states that there is no effect or no difference \n",
    "#   (e.g., the mean is equal to a specific value).\n",
    "\n",
    "#   Alternative Hypothesis (H₁) >> The alternative hypothesis suggests that there is a difference, but it doesn't \n",
    "#   specify whether it is an increase or decrease.\n",
    "\n",
    "#   Test Statistic Distribution >>  In a two-tailed test, you reject the null hypothesis if the test statistic \n",
    "#   falls in either tail of the distribution — both the upper and lower extremes.\n",
    "\n",
    "#2. One-Tailed Test >>>\n",
    "#   Definition >>  A one-tailed test is used when you are testing for the possibility of an effect in only one \n",
    "#   direction (either positive or negative).\n",
    "\n",
    "#   Hypothesis (H₀) >> The null hypothesis typically states that the parameter is equal to a specific value, with\n",
    "#   no effect or no difference.\n",
    "\n",
    "#   Alternative Hypothesis (H₁) >> The alternative hypothesis suggests that there is an effect, and it specifies \n",
    "#   the direction of the effect (either an increase or a decrease).\n",
    "\n",
    "#   Test Statistic Distribution >> in a one-tailed test, you only have a rejection region in one tail (either \n",
    "#   the upper or lower tail of the distribution), depending on the direction of the effect you're testing for.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#8- What is the Z-test, and when is it used in hypothesis testing ?\n",
    "\n",
    "#   The Z-test is a type of statistical test used in hypothesis testing to determine whether there is a significant\n",
    "#   difference between sample data and a population parameter, or between the means of two groups, when the population\n",
    "#   standard deviation is known and the sample size is large (typically n > 30). It is based on the standard normal\n",
    "#   distribution (Z-distribution), which has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "#   When is a Z-test used in hypothesis testing ?\n",
    "#   A Z-test is used in the following situations >>>\n",
    "\n",
    "#1. Testing a population mean when the population standard deviation (σ) is known and the sample size is large(n > 30).\n",
    "\n",
    "#   Example >>  Testing whether the average height of a group of people is equal to a known population mean when \n",
    "#   the population’s standard deviation is known.\n",
    "\n",
    "#2. Comparing two population means when both populations have known standard deviations and the sample size is large.\n",
    "\n",
    "#   Example >> Testing whether the average income of men is different from the average income of women when both \n",
    "#   population standard deviations are known.\n",
    "\n",
    "#3. Testing a proportion when the sample size is large and you know the population proportion.\n",
    "\n",
    "#   Example >> Testing whether the proportion of people who prefer a certain product is different from a known\n",
    "#   population proportion.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#9- How do you calculate the Z-score, and what does it represent in hypothesis testing?\n",
    "\n",
    "#   he Z-score is a measure of how many standard deviations a data point or sample mean is away from the population\n",
    "#   mean. It standardizes the data, allowing comparison across different datasets, even if they have different units\n",
    "#   or scales. In hypothesis testing, the Z-score helps determine how unusual or extreme the observed result is under\n",
    "#   the assumption that the null hypothesis is true.\n",
    "\n",
    "#   FORMULA >> Z = X−μ / σ\n",
    "#   Where >>>>>\n",
    "#   X = the observed value (data point or sample mean),\n",
    "#   μ = population mean (under the null hypothesis),\n",
    "#   σ = population standard deviation.\n",
    "\n",
    "#   What Does the Z-Score Represent?\n",
    "#   The Z-score tells you how far the observed value (or sample mean) is from the population mean in terms of \n",
    "#   standard deviations. Here's how to interpret the Z-score:\n",
    "\n",
    "#   Z = 0: The observed value is exactly equal to the population mean.\n",
    "#   Z > 0: The observed value is greater than the population mean (positive deviation).\n",
    "#   Z < 0: The observed value is less than the population mean (negative deviation).\n",
    "#   The larger the absolute value of the Z-score, the more extreme the observed value is relative to the population\n",
    "#   mean. A Z-score of +2 means the observed value is 2 standard deviations above the mean, while -3 means it's 3 \n",
    "#   standard deviations below the mean.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#10- What is the T-distribution, and when should it be used instead of the normal distribution ?\n",
    "\n",
    "#    The T-distribution (also known as the Student's t-distribution) is a type of probability distribution that\n",
    "#    is used in statistics for hypothesis testing, particularly when dealing with small sample sizes or when the\n",
    "#    population standard deviation is unknown. It is similar to the normal distribution but has heavier tails, \n",
    "#    meaning it accounts for the increased uncertainty that comes with smaller samples.\n",
    "\n",
    "#    When Should You Use the T-Distribution Instead of the Normal Distribution?\n",
    "#    You should use the T-distribution instead of the normal distribution in the following cases----\n",
    "\n",
    "#1.  Small Sample Size >> When your sample size is small (typically n < 30), the sample mean is more likely to be\n",
    "#    far from the population mean, and the normal distribution may not adequately account for the increased variability.\n",
    "#    The T-distribution is more appropriate in these situations because it accounts for the greater uncertainty of the \n",
    "#    sample mean with heavy tails.\n",
    "\n",
    "#2.  Unknown Population Standard Deviation >> When you do not know the population standard deviation (σ) and must \n",
    "#    estimate it from the sample data using the sample standard deviation (s), the T-distribution should be used.\n",
    "#    In contrast, when the population standard deviation is known, you can use the normal distribution.\n",
    "\n",
    "#3.  Estimating Confidence Intervals >> When estimating confidence intervals for the population mean using a small \n",
    "#    sample and an unknown population standard deviation, the t-distribution is used. The confidence interval will \n",
    "#    be wider than that of the normal distribution, reflecting the increased uncertainty.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#11- What is the difference between a Z-test and a T-test ?\n",
    "\n",
    "#    the Z-test and the T-test are both statistical tests used for hypothesis testing, but they are applied under\n",
    "#    different conditions and assumptions. Here's a breakdown of their key differences:\n",
    "\n",
    "#1.  Population Standard Deviation (σ) Known vs. Unknown >>>  \n",
    "#    Z-test: Used when the population standard deviation (σ) is known. It is commonly applied when you have a \n",
    "#    large sample size (n > 30) or when the population standard deviation is available.\n",
    "\n",
    "#    T-test: Used when the population standard deviation is unknown, and you must estimate it using the sample's\n",
    "#    standard deviation (s). T-tests are particularly used when the sample size is small (n < 30).\n",
    "\n",
    "#2.  Sample Size >>>\n",
    "#    Z-test: Generally used for large sample sizes (n > 30). When the sample size is large, the sample mean tends \n",
    "#    to approximate the population mean more closely, making the Z-test more reliable.\n",
    "\n",
    "#    T-test: Typically used for small sample sizes (n < 30). The T-test is designed to handle the increased \n",
    "#    variability and uncertainty that comes with small samples.\n",
    "\n",
    "#3.  Distribution >>>\n",
    "#    Z-test: The Z-test assumes that the sample means are distributed according to a normal distribution \n",
    "#    (or approximately normal if the sample size is large). This is because the Central Limit Theorem applies \n",
    "#    for large samples, so the sampling distribution of the sample mean is normally distributed, even if the \n",
    "#    population distribution is not normal.\n",
    "\n",
    "#    T-test: The T-test assumes that the sample data follows a t-distribution, which is similar to the normal \n",
    "#    distribution but has heavier tails. The t-distribution is used to account for the increased variability when \n",
    "#    estimating the population standard deviation from the sample.\n",
    "\n",
    "#4.  Critical Values and Degrees of Freedom >>>\n",
    "#    Z-test: The Z-test uses critical Z-values, which are constant for any sample size (for example, a two-tailed \n",
    "#    test with α = 0.05 has a critical value of ±1.96).\n",
    "\n",
    "#    T-test: The T-test uses critical T-values, which depend on the degrees of freedom (df). For a one-sample t-test,\n",
    "#    df = n - 1. As the sample size increases, the t-distribution approaches the normal distribution, and the T-value\n",
    "#    becomes similar to the Z-value.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#12- What is the T-test, and how is it used in hypothesis testing ?\n",
    "\n",
    "#    The T-test is a statistical test used to determine whether there is a significant difference between the means \n",
    "#    of two groups or between a sample mean and a known population mean, especially when the sample size is small \n",
    "#    and the population standard deviation is unknown. It is a widely used method in hypothesis testing and is based\n",
    "#    on the T-distribution (Student's t-distribution).\n",
    "\n",
    "#    When to Use the T-Test:\n",
    "#    You would use a T-test in the following scenarios >>\n",
    "\n",
    "#1   When the population standard deviation is unknown.\n",
    "#2   When your sample size is small (typically n < 30).\n",
    "#3   To compare the means of two groups or a sample mean to a known value.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#13- What is the relationship between Z-test and T-test in hypothesis testing ?\n",
    "\n",
    "#    Key Relationship Between the Z-Test and T-Test-----\n",
    "\n",
    "#1.  Underlying Assumptions >>>\n",
    "#    Both tests are used to compare sample means (or sample proportions in some cases) to a population mean \n",
    "#    (or compare the means of two independent groups) in order to determine if the difference is statistically \n",
    "#    significant.\n",
    "\n",
    "#    The Z-test assumes that the population standard deviation (σ) is known, and the sample size is typically large\n",
    "#    (n > 30). In this case, the sample mean follows a normal distribution (due to the Central Limit Theorem).\n",
    "\n",
    "#    The T-test, on the other hand, is used when the population standard deviation is unknown and needs to be \n",
    "#    estimated from the sample data. It is particularly useful for small sample sizes (n < 30).\n",
    "\n",
    "#2.  Distribution >>>\n",
    "\n",
    "#    The Z-test uses the normal distribution (Z-distribution). When the sample size is large, the sample means tend \n",
    "#    to follow a normal distribution regardless of the underlying population distribution (thanks to the Central \n",
    "#    Limit Theorem).\n",
    "\n",
    "#    The T-test uses the t-distribution, which is similar to the normal distribution but has heavier tails to \n",
    "#    account for the increased uncertainty when estimating the population standard deviation from a small sample.\n",
    "#    As the sample size increases, the t-distribution approaches the normal distribution.\n",
    "\n",
    "#3.  Critical Values >>>\n",
    "\n",
    "#    in the Z-test, the critical values (Z-values) depend only on the significance level (α) and are the same for \n",
    "#    any sample size (because the normal distribution doesn't depend on sample size).\n",
    "\n",
    "#    In the T-test, the critical values (T-values) depend on the degrees of freedom (df), which are related to the\n",
    "#    sample size. As the sample size increases, the t-distribution becomes more similar to the normal distribution, \n",
    "#    and the critical T-value approaches the critical Z-value.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#14- What is a confidence interval, and how is it used to interpret statistical results ?\n",
    "\n",
    "#    A confidence interval (CI) is a range of values, derived from a sample, that is used to estimate an unknown\n",
    "#    population parameter (such as the population mean or population proportion). The interval provides a range \n",
    "#    within which we believe the true population parameter is likely to lie, with a certain level of confidence.\n",
    "\n",
    "#    How Confidence Intervals Are Used in Statistical Analysis  >>>> \n",
    "#1.  Estimating Population Parameters >>> Confidence intervals are used to estimate population parameters (like the\n",
    "#    population mean or proportion) based on sample data. This provides a more nuanced understanding than simply \n",
    "#    using a point estimate.\n",
    "\n",
    "#2. Hypothesis Testing >>>\n",
    "#   A confidence interval can be used to test hypotheses. If the null hypothesis value (such as a hypothesized \n",
    "#   population mean) falls outside of the confidence interval, you may reject the null hypothesis.\n",
    "\n",
    "#   For example, if a 95% confidence interval for the mean difference between two groups does not include 0, it \n",
    "#   suggests that there is a statistically significant difference between the groups (at the 5% significance level).\n",
    "\n",
    "#3. Precision and Uncertainty >>> Confidence intervals provide a way to communicate the uncertainty in statistical \n",
    "#   estimates. A wider interval suggests less confidence in the estimate, while a narrower interval suggests more \n",
    "#   precise estimates.\n",
    "\n",
    "#4. Comparing Groups or Treatments >>>  When comparing means from two groups, if their confidence intervals do not\n",
    "#   overlap, it suggests a significant difference between the groups. If the intervals overlap, the difference between\n",
    "#   the groups may not be statistically significant.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#15- What is the margin of error, and how does it affect the confidence interval ?\n",
    "\n",
    "#    The margin of error (MoE) is a measure of the uncertainty or precision of an estimate in statistical analysis. \n",
    "#    It represents the amount added or subtracted from the point estimate (such as a sample mean or proportion) to \n",
    "#    create a confidence interval. In simpler terms, it defines the range within which the true population parameter\n",
    "#   (e.g., mean, proportion) is likely to fall, based on the sample data.\n",
    "\n",
    "#    How Margin of Error Affects the Confidence Interval >>>\n",
    "#    The margin of error determines how wide or narrow the confidence interval will be.\n",
    "\n",
    "#    A larger margin of error results in a wider confidence interval, which reflects more uncertainty in the estimate.\n",
    "#    A smaller margin of error results in a narrower confidence interval, which reflects greater precision in the estimate.\n",
    "#    Increasing the sample size reduces the margin of error, making the confidence interval narrower and the estimate \n",
    "#    more precise. This is because larger samples provide more information about the population.\n",
    "\n",
    "#    Increasing the confidence level (e.g., from 95% to 99%) increases the margin of error and thus makes the \n",
    "#    confidence interval wider. This is because you want to be more certain that the interval contains the true \n",
    "#    population parameter, so you expand the range.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#16- How is Bayes' Theorem used in statistics, and what is its significance ?\n",
    "\n",
    "#    Bayes' Theorem is a fundamental concept in statistics that describes how to update the probability of a \n",
    "#    hypothesis based on new evidence or data. It is named after the Reverend Thomas Bayes, and it's a powerful\n",
    "#    tool in probability theory and statistical inference, especially in situations where we need to revise our \n",
    "#    beliefs or knowledge about an event or hypothesis as more information becomes available.\n",
    "\n",
    "#    FORMULA --  P(A∣B) =  P(B∣A)⋅P(A) / P(B)\n",
    "#    Where:\n",
    "#    P(A∣B) is the posterior probability, which is the probability of event A happening given the evidence B.\n",
    "#    P(B∣A) is the likelihood, which is the probability of observing evidence B given that event A happens.\n",
    "#    P(A) is the prior probability, which is the initial probability of event A happening before seeing the evidence.\n",
    "#    P(B) is the marginal likelihood or the total probability of evidence B across all possible events, often calculated \n",
    "#    as the sum of the likelihoods for all possible hypotheses.\n",
    "\n",
    "#    Significance of Bayes' Theorem in Statistics >>>\n",
    "\n",
    "#1.  Updating Probabilities >>>  Bayes’ Theorem is crucial in updating probabilities as new evidence is acquired. \n",
    "#    It’s widely used in fields like machine learning, medicine, and decision analysis, where continuous learning or\n",
    "#    data collection is needed to improve predictions.\n",
    "\n",
    "#2.  Real-World Applications >>> \n",
    "\n",
    "#    Medical Diagnosis >> It helps estimate the probability of a disease based on test results, incorporating both \n",
    "#    the test’s accuracy and the prevalence of the disease.\n",
    "\n",
    "#    Spam Filtering >> In email spam filters, Bayes' Theorem can be used to calculate the probability that an\n",
    "#    email is spam, based on words or patterns it contains.\n",
    "\n",
    "#    Risk Assessment >> In finance and insurance, Bayes’ Theorem is used to evaluate risks and adjust probabilities \n",
    "#    of events (like a financial crash or insurance claims) based on changing data.\n",
    "\n",
    "#3.  Decision Making >> It is used for decision-making under uncertainty. By continuously updating our beliefs about\n",
    "#    the likelihood of outcomes, we can make more informed and rational decisions in situations with incomplete information.\n",
    "\n",
    "#4.  Machine Learning >> In machine learning, Bayesian inference allows models to update predictions and incorporate\n",
    "#    new data, making it a key tool in fields like natural language processing and reinforcement learning. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#17- What is the Chi-square distribution, and when is it used ?\n",
    "\n",
    "#   The Chi-square distribution distribution) is a type of probability distribution that is widely used in statistical\n",
    "#   inference, particularly in hypothesis testing and in the construction of confidence intervals. It is a special \n",
    "#   case of the gamma distribution and is particularly useful for tests involving categorical data.\n",
    "\n",
    "#   When Is the Chi-square Distribution Used?\n",
    "#   The Chi-square distribution is commonly used in two main areas of statistics: goodness-of-fit tests and tests\n",
    "#   for independence. Here are the main applications:\n",
    "\n",
    "#1. Chi-square Goodness-of-Fit Test >> This test is used to determine whether an observed frequency distribution \n",
    "#   differs from a theoretical distribution. For example, you might want to test if a die is fair (i.e., if the \n",
    "#   outcomes are equally likely).\n",
    "\n",
    "#   Null Hypothesis >> The observed data follows the expected distribution.\n",
    "#   Alternative Hypothesis >> The observed data does not follow the expected distribution.\n",
    "\n",
    "#   Example >>  A researcher wants to test if a die is fair. They roll the die 60 times and record the number of \n",
    "#   times each face appears. They then use the Chi-square goodness-of-fit test to compare the observed frequencies \n",
    "#   to the expected frequencies (which should be 10 for each face of the die, assuming fairness).\n",
    "\n",
    "#2. Chi-square Test for Independence >> This test is used to determine if two categorical variables are independent \n",
    "#   of each other. It compares the observed frequencies of occurrences of combinations of categories with the expected\n",
    "#   frequencies if the variables were independent.\n",
    "\n",
    "#   Null Hypothesis >> The two variables are independent.\n",
    "#   Alternative Hypothesis >> The two variables are dependent (i.e., there is an association between the two variables).\n",
    "\n",
    "#   Example >>  A researcher might want to test if gender (male/female) is independent of preference for a type of \n",
    "#   music (classical/jazz/rock). The Chi-square test for independence would be used to assess if the distribution of \n",
    "#   music preferences is independent of gender.\n",
    "\n",
    "#3.  Chi-square Test for Homogeneity >> This test is used to determine if different populations (or groups) have the\n",
    "#    same distribution of a categorical variable. It's similar to the test for independence, but it involves multiple \n",
    "#    groups.\n",
    "\n",
    "#   Example >>  A company might want to test if customers from different regions have the same preference for a \n",
    "#   product. The Chi-square test for homogeneity would allow them to compare the distribution of product preferences\n",
    "#   across different regions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#18- What is the Chi-square goodness of fit test, and how is it applied ?\n",
    "\n",
    "\n",
    "#   The Chi-square goodness-of-fit test is a statistical test used to determine whether the observed frequency \n",
    "#   distribution of a categorical variable matches an expected distribution. This test is used when you want to \n",
    "#   test how well the observed data fits a particular theoretical or expected distribution, often to assess whether\n",
    "#   the data follows a specific pattern or distribution. \n",
    "\n",
    "#   Steps in Performing a Chi-square Goodness-of-Fit Test:\n",
    "#   State the hypotheses:\n",
    "\n",
    "#   H0: The observed distribution fits the expected distribution.\n",
    "#   H1: The observed distribution does not fit the expected distribution.\n",
    "#   Set the significance level (α): Typically, this is set at 0.05 (5%), but it can vary depending on the situation.\n",
    "#   calculate the expected frequencies: If you know the total sample size and the expected proportion for each\n",
    "#   category, calculate the expected frequencies by multiplying the total number of observations by the expected\n",
    "#   proportion for each category.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#19- What is the F-distribution, and when is it used in hypothesis testing ?\n",
    "\n",
    "#    The F-distribution is a probability distribution that arises frequently in statistics, particularly in the \n",
    "#    context of variance analysis and hypothesis testing. It is used when comparing two variances or when working \n",
    "#    with regression models and analysis of variance (ANOVA). The F-distribution is a continuous probability\n",
    "#    distribution that is skewed to the right and has two parameters: degrees of freedom associated with the \n",
    "#    numerator and denominator.\n",
    "\n",
    "#    When is the F-distribution used in hypothesis testing?\n",
    "#    The F-distribution is used in several key areas of hypothesis testing, primarily in contexts that involve \n",
    "#    variances or multiple group comparisons. Below are the main applications:\n",
    "\n",
    "#1.  Analysis of Variance (ANOVA) >>>>\n",
    "\n",
    "#    One-way ANOVA >> Tests whether the means of three or more groups are equal. The F-statistic is used to compare\n",
    "#    the within-group variance to the between-group variance. A large F-statistic suggests that the group means are \n",
    "#    significantly different.\n",
    "\n",
    "#    Two-way ANOVA >> Similar to one-way ANOVA but with two factors. It is used to assess the interaction between\n",
    "#    the two factors and their individual effects on the response variable.\n",
    "\n",
    "#    Null Hypothesis (H₀) >> All group means are equal.\n",
    "\n",
    "#    Alternative Hypothesis (H₁) >>  At least one group mean is different.\n",
    "\n",
    "#    Example >>  A researcher wants to compare the average test scores of students from three different teaching\n",
    "#    methods. The F-distribution will be used to determine if the mean scores are significantly different across\n",
    "#    the methods.\n",
    "\n",
    "#2.  F-test for Equality of Variances >>>>\n",
    "#    The F-distribution is used to compare the variances of two populations to see if they are significantly different.\n",
    "#    This is particularly useful when checking assumptions for other statistical tests (such as t-tests).\n",
    "\n",
    "#    Null Hypothesis (H₀) >>  The two populations have the same variance.\n",
    "\n",
    "#    Alternative Hypothesis (H₁) >> The two populations have different variances.\n",
    "\n",
    "#    Example >>  A company wants to compare the variability in the production times of two machines. The F-test \n",
    "#    can be used to test if the variances of the two machines’ production times are the same.\n",
    "\n",
    "#3.  Regression Analysis >>>>\n",
    "#    In multiple regression analysis, the F-test is used to determine whether the overall regression model is a \n",
    "#    good fit for the data. Specifically, it tests whether at least one of the predictors is significantly related\n",
    "#    to the outcome variable.\n",
    "\n",
    "#    Null Hypothesis (H₀) >>  The model with all the predictors does not explain the variance in the dependent \n",
    "#    variable (i.e., all coefficients are zero).\n",
    "\n",
    "#    Alternative Hypothesis (H₁) >>  At least one predictor variable has a non-zero coefficient.\n",
    "\n",
    "#    Example >>  In a study examining the relationship between income, education, and job satisfaction, the F-test \n",
    "#    can determine if the combined effect of income and education significantly predicts job satisfaction.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#20- What is an ANOVA test, and what are its assumptions ?\n",
    "\n",
    "#    An ANOVA (Analysis of Variance) test is a statistical method used to compare the means of three or more groups \n",
    "#    to determine if there is a statistically significant difference between them. Instead of conducting multiple \n",
    "#    t-tests for each pair of groups (which would increase the risk of Type I error), ANOVA allows for a single test \n",
    "#    to evaluate whether at least one group mean is different from the others.\n",
    "\n",
    "#    Assumptions of ANOVA >>>\n",
    "#    For ANOVA to provide valid results, certain assumptions must be met >>> \n",
    "\n",
    "#1.  Independence >>>  The observations must be independent of one another. This means that the value of one \n",
    "#    observation in a group should not influence the value of another observation.\n",
    "#    This assumption is particularly important when the data come from separate groups or experimental conditions.\n",
    "\n",
    "#2.  Normality >>> The data in each group should be approximately normally distributed. This assumption ensures\n",
    "#    that the F-distribution approximation is accurate.\n",
    "#    While ANOVA is fairly robust to violations of normality when the sample sizes are large (thanks to the Central \n",
    "#    Limit Theorem), it is important to check for normality with smaller sample sizes.\n",
    "   \n",
    "#3.  Homogeneity of Variances (Homogeneity of Variance or Homoscedasticity) >> The variances in each group should \n",
    "#    be approximately equal. This assumption is important because if the variances are vastly different across groups, \n",
    "#    the F-test may not be valid.\n",
    "#    This assumption can be tested using a Levene’s test or Bartlett’s test for equal variances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#21- What are the different types of ANOVA tests ?\n",
    "\n",
    "#    The main types of ANOVA tests are >>>> \n",
    "\n",
    "#1.  One-Way ANOVA (also called One-Factor ANOVA) ----\n",
    "#    Purpose >> Used to compare the means of three or more independent groups based on one independent variable(factor).\n",
    "#    Example: Comparing the average test scores of students from three different teaching methods \n",
    "#    (Method A, Method B, and Method C).\n",
    "\n",
    "#    Assumptions >>\n",
    "#    Independence of observations (each group is independent of the others).\n",
    "#    Normality (the data in each group should be approximately normally distributed).\n",
    "#    Homogeneity of variances (the variances of the groups should be approximately equal).\n",
    "\n",
    "#    Hypotheses >>\n",
    "#    H0 > The means of all groups are equal (no significant difference).\n",
    "#    H1 >  At least one group mean is different from the others.\n",
    "\n",
    "#    Example >> A study investigates three different diets (A, B, C) to see which leads to the greatest weight loss.\n",
    "#    The weight loss for each group (A, B, and C) is measured. One-way ANOVA is used to determine if the average\n",
    "#    weight loss differs significantly between the three diets.\n",
    "\n",
    "#2.  Two-Way ANOVA (also called Two-Factor ANOVA) -----\n",
    "#    Purpose >>  Used to examine the effect of two independent variables (factors) on a dependent variable.\n",
    "#    It also assesses whether there is an interaction between the two factors.\n",
    "#    Example >>  A study on how teaching method (Method A, Method B) and study time (short, medium, long) affect students' test scores.\n",
    "\n",
    "#    Assumptions >>\n",
    "#    Independence of observations.\n",
    "#    Normality of the residuals.\n",
    "#    Homogeneity of variances (equal variances across groups).\n",
    "\n",
    "#    Hypotheses >>\n",
    "#    Main effects ----\n",
    "#    𝐻0: There is no significant effect of Factor 1 (e.g., teaching method) on the dependent variable.\n",
    "#    𝐻1: There is a significant effect of Factor 1.\n",
    "#    𝐻0: There is no significant effect of Factor 2 (e.g., study time) on the dependent variable.\n",
    "#    𝐻1: There is a significant effect of Factor 2.\n",
    "\n",
    "#    EXAMPLE >> \n",
    "#    In a study examining the effect of diet (low-carb, high-carb) and exercise type (aerobic, strength training)\n",
    "#    on weight loss, two-way ANOVA is used to evaluate:\n",
    "\n",
    "#    The effect of diet on weight loss.\n",
    "#    The effect of exercise type on weight loss.\n",
    "#    The interaction between diet and exercise type on weight loss.\n",
    "\n",
    "\n",
    "#3.  Repeated Measures ANOVA ----\n",
    "#    Purpose >> Used when the same subjects are measured multiple times (repeated measurements) under different \n",
    "#    conditions or over time. This test accounts for the fact that the observations are not independent,\n",
    "#    as they come from the same individuals.\n",
    "\n",
    "#    Example >> Measuring blood pressure in patients at three different times: before treatment, after 1 month, and after 3 months.\n",
    "#    Assumptions >>> \n",
    "#    Sphericity (the variances of the differences between all possible pairs of groups should be roughly equal).\n",
    "#    This assumption is related to the correlation structure between repeated measures.\n",
    "\n",
    "#    Normality of residuals >>> \n",
    "#    Hypotheses >> \n",
    "#    𝐻0: The means for the repeated measurements are equal (no significant difference over time).\n",
    "#    𝐻1: At least one measurement differs significantly from the others.\n",
    "\n",
    "#    Example >> A study measures a patient’s anxiety level at four different time points: before treatment,\n",
    "#    after 1 week, 2 weeks, and 3 weeks of therapy. Repeated measures ANOVA is used to determine if there is \n",
    "#    a significant change in anxiety levels over time.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#22- What is the F-test, and how does it relate to hypothesis testing ?\n",
    "\n",
    "#    The F-test is a statistical test used to compare two or more variances to determine if they are significantly\n",
    "#    different from each other. It is widely used in the context of ANOVA (Analysis of Variance), regression analysis,\n",
    "#    and other statistical models to test hypotheses about variances or to evaluate the goodness of fit of a model.\n",
    "\n",
    "#    The F-test is directly related to hypothesis testing because it is a statistical method used to test specific\n",
    "#    hypotheses about variances or means in various contexts, such as ANOVA, regression analysis, and variance \n",
    "#    comparisons. Here's how it fits into the framework of hypothesis testing:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                                     ---------Practical-----------\n",
    "\n",
    "#1.  Write a Python program to perform a Z-test for comparing a sample mean to a known population mean \n",
    "#    and interpret the results.\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "sample_data = [85, 90, 92, 87, 88, 91, 93, 89, 94, 86]\n",
    "sample_mean = np.mean(sample_data)\n",
    "sample_size = len(sample_data)\n",
    "\n",
    "population_mean = 90\n",
    "population_std = 3  \n",
    "\n",
    "z_score = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))\n",
    "\n",
    "p_value = 2 * (1 - stats.norm.cdf(abs(z_score))) \n",
    "\n",
    "print(f\"Sample Mean: {sample_mean:.2f}\")\n",
    "print(f\"Z-Score: {z_score:.2f}\")\n",
    "print(f\"P-Value: {p_value:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the sample mean and the population mean.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the sample mean and the population mean.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2. Simulate random data to perform hypothesis testing and calculate the corresponding P-value using Python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)  \n",
    "sample_size = 50\n",
    "population_mean = 100  \n",
    "population_std = 15 \n",
    "\n",
    "sample_data = np.random.normal(population_mean, population_std, sample_size)\n",
    "\n",
    "sample_mean = np.mean(sample_data)\n",
    "sample_std = np.std(sample_data, ddof=1)  \n",
    "\n",
    "z_score = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))\n",
    "\n",
    "#Calculate the P-value (two-tailed test)\n",
    "p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))  \n",
    "\n",
    "print(f\"Sample Mean: {sample_mean:.2f}\")\n",
    "print(f\"Z-Score: {z_score:.2f}\")\n",
    "print(f\"P-Value: {p_value:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the sample mean and the population mean.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the sample mean and the population mean.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#3. Implement a one-sample Z-test using Python to compare the sample mean with the population mean ?\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "population_mean = 100  \n",
    "population_std = 15 \n",
    "sample_size = 50 \n",
    "\n",
    "np.random.seed(42) \n",
    "sample_data = np.random.normal(population_mean, population_std, sample_size)\n",
    "sample_mean = np.mean(sample_data)\n",
    "\n",
    "z_score = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))\n",
    "\n",
    "p_value = 1 - stats.norm.cdf(z_score)\n",
    "\n",
    "print(f\"Sample Mean: {sample_mean:.2f}\")\n",
    "print(f\"Z-Score: {z_score:.2f}\")\n",
    "print(f\"P-Value (Right-tailed test): {p_value:.4f}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: The sample mean is significantly greater than the population mean.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The sample mean is not significantly greater than the population mean.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#4. Perform a two-tailed Z-test using Python and visualize the decision region on a plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "population_mean = 100 \n",
    "population_std = 15   \n",
    "sample_size = 50      \n",
    "\n",
    "np.random.seed(42) \n",
    "sample_data = np.random.normal(population_mean, population_std, sample_size)\n",
    "sample_mean = np.mean(sample_data)\n",
    "\n",
    "z_score = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))\n",
    "\n",
    "alpha = 0.05  \n",
    "z_critical = stats.norm.ppf(1 - alpha / 2)\n",
    "\n",
    "p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))  \n",
    "if abs(z_score) > z_critical:\n",
    "    decision = \"Reject the null hypothesis: There is a significant difference.\"\n",
    "else:\n",
    "    decision = \"Fail to reject the null hypothesis: There is no significant difference.\"\n",
    "\n",
    "x = np.linspace(-4, 4, 1000) \n",
    "y = stats.norm.pdf(x, 0, 1) \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, label=\"Standard Normal Distribution\")\n",
    "plt.fill_between(x, y, where=(x < -z_critical), color='red', alpha=0.5, label=\"Rejection Region (Left Tail)\")\n",
    "plt.fill_between(x, y, where=(x > z_critical), color='red', alpha=0.5, label=\"Rejection Region (Right Tail)\")\n",
    "plt.axvline(z_score, color='green', linestyle='dashed', label=f'Z-Score = {z_score:.2f}')\n",
    "plt.axvline(z_critical, color='black', linestyle='dashed', label=f'Critical Z-Value = ±{z_critical:.2f}')\n",
    "plt.axvline(-z_critical, color='black', linestyle='dashed')\n",
    "plt.title(\"Two-Tailed Z-Test and Decision Region\")\n",
    "plt.xlabel(\"Z-Score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sample Mean: {sample_mean:.2f}\")\n",
    "print(f\"Z-Score: {z_score:.2f}\")\n",
    "print(f\"P-Value: {p_value:.4f}\")\n",
    "print(decision)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#5. Create a Python function that calculates and visualizes Type 1 and Type 2 errors during hypothesis testing \n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "population_mean = 100 \n",
    "population_std = 15   \n",
    "sample_size = 50       \n",
    "alt_mean = 105         \n",
    "\n",
    "alpha = 0.05 \n",
    "z_critical = stats.norm.ppf(1 - alpha / 2)\n",
    "\n",
    "standard_error = population_std / np.sqrt(sample_size)\n",
    "\n",
    "rejection_region_left = population_mean - z_critical * standard_error\n",
    "rejection_region_right = population_mean + z_critical * standard_error\n",
    "\n",
    "x = np.linspace(population_mean - 4 * population_std, population_mean + 4 * population_std, 1000)\n",
    "\n",
    "y_null = stats.norm.pdf(x, population_mean, population_std) \n",
    "y_alt = stats.norm.pdf(x, alt_mean, population_std) \n",
    "\n",
    "plt.plot(x, y_null, label=f'Null Hypothesis (μ={population_mean})', color='blue')\n",
    "plt.plot(x, y_alt, label=f'Alternative Hypothesis (μ={alt_mean})', color='green', linestyle='dashed')\n",
    "plt.fill_between(x, y_null, where=(x < rejection_region_left) | (x > rejection_region_right),\n",
    "                 color='red', alpha=0.5, label=\"Type 1 Error (False Positive)\")\n",
    "plt.fill_between(x, y_alt, where=(x > rejection_region_left) & (x < rejection_region_right),\n",
    "                 color='yellow', alpha=0.5, label=\"Type 2 Error (False Negative)\")\n",
    "plt.axvline(rejection_region_left, color='black', linestyle='dashed', label='Critical Value Left')\n",
    "plt.axvline(rejection_region_right, color='black', linestyle='dashed', label='Critical Value Right')\n",
    "plt.axvline(population_mean, color='blue', linestyle='solid', label=f'Population Mean = {population_mean}')\n",
    "plt.title(\"Visualization of Type 1 and Type 2 Errors in Hypothesis Testing\")\n",
    "plt.xlabel(\"Value of Sample Mean (X̄)\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Critical Z-Value: ±{z_critical:.2f}\")\n",
    "print(f\"Rejection Region Left: {rejection_region_left:.2f}\")\n",
    "print(f\"Rejection Region Right: {rejection_region_right:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#6- Write a Python program to perform an independent T-test and interpret the results.\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "group1 = np.random.normal(loc=100, scale=15, size=30) \n",
    "group2 = np.random.normal(loc=105, scale=20, size=30) \n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "\n",
    "alpha = 0.05  \n",
    "if p_value < alpha:\n",
    "    decision = \"Reject the null hypothesis: There is a significant difference between the groups.\"\n",
    "else:\n",
    "    decision = \"Fail to reject the null hypothesis: There is no significant difference between the groups.\"\n",
    "\n",
    "print(f\"T-Statistic: {t_stat:.4f}\")\n",
    "print(f\"P-Value: {p_value:.4f}\")\n",
    "print(decision)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#7. Perform a paired sample T-test using Python and visualize the comparison results\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)  \n",
    "\n",
    "before_treatment = np.random.normal(loc=100, scale=10, size=30) \n",
    "after_treatment = before_treatment + np.random.normal(loc=5, scale=5, size=30)  \n",
    "\n",
    "t_stat, p_value = stats.ttest_rel(before_treatment, after_treatment)\n",
    " \n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    decision = \"Reject the null hypothesis: There is a significant difference between before and after treatment.\"\n",
    "else:\n",
    "    decision = \"Fail to reject the null hypothesis: There is no significant difference between before and after treatment.\"\n",
    "\n",
    "print(f\"T-Statistic: {t_stat:.4f}\")\n",
    "print(f\"P-Value: {p_value:.4f}\")\n",
    "print(decision)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(before_treatment, label='Before Treatment', marker='o', linestyle='--', color='blue')\n",
    "plt.plot(after_treatment, label='After Treatment', marker='o', linestyle='-', color='green')\n",
    "plt.title(\"Comparison of Before and After Treatment\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Measurement Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#8. Simulate data and perform both Z-test and T-test, then compare the results using Python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)  \n",
    "\n",
    "population_mean = 100\n",
    "population_std = 15\n",
    "\n",
    "sample_size = 50\n",
    "sample_data = np.random.normal(loc=100, scale=15, size=sample_size)\n",
    "\n",
    "sample_mean = np.mean(sample_data)\n",
    "z_stat = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))\n",
    "z_p_value = 2 * (1 - stats.norm.cdf(abs(z_stat))) \n",
    "t_stat, t_p_value = stats.ttest_1samp(sample_data, population_mean)\n",
    "\n",
    "alpha = 0.05  \n",
    "z_decision = \"Reject the null hypothesis\" if z_p_value < alpha else \"Fail to reject the null hypothesis\"\n",
    "t_decision = \"Reject the null hypothesis\" if t_p_value < alpha else \"Fail to reject the null hypothesis\"\n",
    "\n",
    "print(f\"Z-Test: Z-Statistic = {z_stat:.4f}, P-Value = {z_p_value:.4f}, Decision: {z_decision}\")\n",
    "print(f\"T-Test: T-Statistic = {t_stat:.4f}, P-Value = {t_p_value:.4f}, Decision: {t_decision}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(sample_data, bins=15, color='lightblue', edgecolor='black', alpha=0.7, label='Sample Data')\n",
    "plt.axvline(population_mean, color='red', linestyle='dashed', label=f'Population Mean = {population_mean}')\n",
    "plt.axvline(sample_mean, color='green', linestyle='dashed', label=f'Sample Mean = {sample_mean:.2f}')\n",
    "plt.title(\"Histogram of Sample Data with Population and Sample Means\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#9. Write a Python function to calculate the confidence interval for a sample mean and explain its significance\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "sample_data= [4,6,8,10,12]\n",
    "sample_mean= np.mean(sample_data)\n",
    "sample_std= np.std(sample_data, ddof=1)\n",
    "\n",
    "data_size= len(sample_data)\n",
    "#degree of freedom \n",
    "df= data_size - 1\n",
    "confidence_level= 0.95\n",
    "t_value= stats.t.ppf((1+confidence_level) / 2, df)\n",
    "print(\"t_value is\", t_value)\n",
    "\n",
    "margin_of_error= t_value *(sample_std / np.sqrt(data_size))\n",
    "print(\"margin_of_error is\",margin_of_error )\n",
    "\n",
    "confidence_interval= (sample_mean-margin_of_error, sample_mean + margin_of_error)\n",
    "print(f\"confidence interval is\", confidence_interval)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#10- Write a Python program to calculate the margin of error for a given confidence level using sample data\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "sample_data= [2,4,6,8,10,12,14]\n",
    "data_size= len(sample_data)\n",
    "\n",
    "sample_mean= np.mean(sample_data)\n",
    "sample_std= np.std(sample_data, ddof=1)\n",
    "\n",
    "confidence_level= 0.95\n",
    "degrees_of_freedom = data_size- 1\n",
    "\n",
    "t_value= stats.t.ppf((1+confidence_level)/ 2, degrees_of_freedom)\n",
    "\n",
    "margin_of_error= t_value * (sample_std/ np.sqrt(data_size))\n",
    "\n",
    "print(\"size of data is\", data_size)\n",
    "print(\"t_value is\", t_value)\n",
    "print(\"margin of error is\", margin_of_error)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#11- Implement a Bayesian inference method using Bayes' Theorem in Python and explain the process\n",
    "# probabilities\n",
    "P_H = 0.01  # Prior: Probability that a person has the disease\n",
    "P_not_H = 1 - P_H  # Probability that a person does not have the disease\n",
    "\n",
    "P_E_given_H = 0.95  # Likelihood: Probability of a positive test given the person has the disease\n",
    "P_E_given_not_H = 0.05  # Likelihood: Probability of a positive test given the person does not have the disease\n",
    "\n",
    "# P(E) = P(E|H) * P(H) + P(E|¬H) * P(¬H)\n",
    "P_E = P_E_given_H * P_H + P_E_given_not_H * P_not_H\n",
    "\n",
    "# P(H|E) = (P(E|H) * P(H)) / P(E)\n",
    "P_H_given_E = (P_E_given_H * P_H) / P_E\n",
    "\n",
    "print(\"Prior Probability (P(H)):\", P_H)\n",
    "print(\"Likelihood (P(E|H)):\", P_E_given_H)\n",
    "print(\"Marginal Likelihood (P(E)):\", P_E)\n",
    "print(\"Posterior Probability (P(H|E)):\", P_H_given_E)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#12- Perform a Chi-square test for independence between two categorical variables in Python\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Contingency table for Gender and Smoking Habit\n",
    "data = np.array([[40, 60],  # Male\n",
    "                 [30, 70]])  # Female\n",
    "\n",
    "# Perform Chi-square test for independence\n",
    "chi2, p_value, dof, expected = stats.chi2_contingency(data)\n",
    "\n",
    "print(f\"Chi-square statistic: {chi2}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "print(f\"Expected frequencies table:\\n{expected}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant relationship between gender and smoking habit.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant relationship between gender and smoking habit.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#13- Write a Python program to calculate the expected frequencies for a Chi-square test based on observed data\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "observed_data= np.array([[50, 30],\n",
    "                         [70, 50]])\n",
    "\n",
    "total_rows= observed_data.sum(axis=1)\n",
    "total_columns= observed_data.sum(axis=0)\n",
    "total= observed_data.sum()\n",
    "\n",
    "expected_data= np.outer(total_rows, total_columns)/ total\n",
    "\n",
    "print(\"observed data is\", observed_data)\n",
    "print(\"expected data is\", expected_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#14- Perform a goodness-of-fit test using Python to compare the observed data to an expected distribution\n",
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "# Observed frequencies\n",
    "observed = np.array([48, 52])  # 48 heads, 52 tails\n",
    "\n",
    "# Expected frequencies )\n",
    "expected = np.array([50, 50])\n",
    "\n",
    "chi2_stat, p_value = chisquare(observed, expected)\n",
    "\n",
    "print(f\"Chi-square statistic: {chi2_stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05 \n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: The coin is not fair.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The coin is fair.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#15- Create a Python script to simulate and visualize the Chi-square distribution and discuss its characteristics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "num_samples= 1000\n",
    "\n",
    "df_values= [1,2,5,10,20]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "for df in df_values:\n",
    "    chi_square_data= np.random.chisquare(df, num_samples)\n",
    "\n",
    "#visualization\n",
    "plt.hist(chi_square_data, bins= 20, color= 'pink', edgecolor= 'black')\n",
    "plt.title(\"distribution of chi square\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#Characteristics of the Chi-Square Distribution from the Visualization:\n",
    "#For df = 1: the distribution is highly right-skewed, with a sharp peak near 0. This is typical for small degrees of freedom.\n",
    "#For df = 2:the distribution starts to look less skewed, but there is still some skewness.\n",
    "#For df = 5:The Chi-square distribution starts to flatten out. The peak moves further right as the mean increases to 5 (df value).\n",
    "#For df = 10:The distribution becomes more symmetric and bell-shaped. It is closer to a normal distribution.\n",
    "#For df = 20:The distribution is almost symmetric and closely resembles a normal distribution. This illustrates that\n",
    "#            as the degrees of freedom increase, the Chi-square distribution becomes less skewed and more normal.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#16- Implement an F-test using Python to compare the variances of two random samples\n",
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "\n",
    "np.random.seed(42)\n",
    "sample1 = np.random.normal(loc=0, scale=1, size=100)\n",
    "sample2 = np.random.normal(loc=0, scale=2, size=100)\n",
    "\n",
    "var1 = np.var(sample1, ddof=1)\n",
    "var2 = np.var(sample2, ddof=1)\n",
    "\n",
    "F_statistic = var1 / var2 if var1 > var2 else var2 / var1\n",
    "\n",
    "df1 = len(sample1) - 1\n",
    "df2 = len(sample2) - 1\n",
    "\n",
    "p_value = 1 - f.cdf(F_statistic, df1, df2)\n",
    "\n",
    "print(f\"Variance of sample1: {var1}\")\n",
    "print(f\"Variance of sample2: {var2}\")\n",
    "print(f\"F-statistic: {F_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: The variances are significantly different.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The variances are not significantly different.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#17- Write a Python program to perform an ANOVA test to compare means between multiple groups and interpret the results\n",
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "np.random.seed(42)\n",
    "group1= np.random.normal(loc=50, scale= 10, size= 30)\n",
    "group2= np.random.normal(loc=55, scale= 15, size= 30)\n",
    "group3= np.random.normal(loc=60, scale= 20, size= 30)\n",
    "\n",
    "stat, p_value= f_oneway(group1, group2, group3)\n",
    "\n",
    "print(\"annova values are\", stat)\n",
    "print(\"p_values are\", p_value)\n",
    "\n",
    "#interpretation of the result\n",
    "if  p_value < 0.05 :\n",
    "    print(\"reject the null hypothesis\")\n",
    "else:\n",
    "    print(\"fail to reject the null hypothesis\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#18- Perform a one-way ANOVA test using Python to compare the means of different groups and plot the results\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "group1= np.random.normal(loc= 30, scale= 10, size= 20)\n",
    "group2= np.random.normal(loc= 35, scale= 15, size= 20)\n",
    "group3= np.random.normal(loc= 40, scale= 20, size= 20)\n",
    "\n",
    "stat, p_value= f_oneway(group1, group2, group3)\n",
    "\n",
    "print(\"annova\", stat)\n",
    "print(\"p_value\", p_value)\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"reject the null hypothesis\")\n",
    "else:\n",
    "    print(\"fail to rejcet the null hypothesis\")\n",
    "\n",
    "#visualization > \n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=['group1',' group2', 'group3'],notch=True, patch_artist=True )\n",
    "plt.title(\"boxplot of groups\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#19- Write a Python function to check the assumptions (normality, independence, and equal variance) for ANOVA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Function to check the assumptions for ANOVA:\n",
    "def check_anova_assumptions_simple(groups):\n",
    "    \"\"\"\n",
    "    Check the normality (Shapiro-Wilk test) and equal variance (Levene's test) assumptions for ANOVA.\n",
    "    \n",
    "    Parameters:\n",
    "    - groups: list of arrays or lists, where each element represents a group of data.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nNormality Check (Q-Q Plot):\")\n",
    "    for i, group in enumerate(groups):\n",
    "        stats.probplot(group, dist=\"norm\", plot=plt)\n",
    "        plt.title(f\"Q-Q Plot for Group {i+1}\")\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\nEqual Variance Check (Levene's Test):\")\n",
    "    stat, p_value = stats.levene(*groups)\n",
    "    print(f\"Levene's test statistic = {stat:.4f}, p-value = {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"  -> Reject null hypothesis: The variances are not equal.\")\n",
    "    else:\n",
    "        print(\"  -> Fail to reject null hypothesis: The variances are equal.\")\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "group1 = np.random.normal(loc=50, scale=10, size=30)  # Group 1\n",
    "group2 = np.random.normal(loc=55, scale=12, size=30)  # Group 2\n",
    "group3 = np.random.normal(loc=60, scale=15, size=30)  # Group 3\n",
    "\n",
    "groups = [group1, group2, group3]\n",
    "\n",
    "check_anova_assumptions_simple(groups)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#20- Perform a two-way ANOVA test using Python to study the interaction between two factors and visualize the results\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "factor1 = ['A', 'A', 'B', 'B'] * 10\n",
    "factor2 = ['X', 'Y', 'X', 'Y'] * 10\n",
    "response = np.random.normal(loc=10, scale=5, size=40) + \\\n",
    "           np.where(np.array(factor1) == 'A', 5, 0) + \\\n",
    "           np.where(np.array(factor2) == 'X', 3, 0)\n",
    "data = pd.DataFrame({'Factor1': factor1, 'Factor2': factor2, 'Response': response})\n",
    "\n",
    "model = ols('Response ~ C(Factor1) + C(Factor2) + C(Factor1):C(Factor2)', data=data).fit()\n",
    "anova_results = sm.stats.anova_lm(model, typ=2)\n",
    "print(anova_results)\n",
    "\n",
    "sns.boxplot(x='Factor1', y='Response', hue='Factor2', data=data)\n",
    "plt.title('Boxplot of Response by Factor1 and Factor2')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#21- Write a Python program to visualize the F-distribution and discuss its use in hypothesis testing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import f\n",
    "\n",
    "dfn= 2\n",
    "dfd= 10\n",
    "\n",
    "x= np.linspace(0,5,500)\n",
    "\n",
    "y= f.pdf(x, dfn, dfd)\n",
    "\n",
    "#visualization \n",
    "plt.plot(x, y, label=f'F-distribution (dfn={dfn}, dfd={dfd})')\n",
    "plt.title('F-distribution')\n",
    "plt.xlabel('F-statistic')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#22- Perform a one-way ANOVA test in Python and visualize the results with boxplots to compare group means.\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import stats\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "group1= np.random.normal(10,5,30)\n",
    "group2= np.random.normal(12,5,30)\n",
    "group3= np.random.normal(15,5,30)\n",
    "\n",
    "#one way annova\n",
    "f_stats, p_value= stats.f_oneway(group1, group2, group3)\n",
    "\n",
    "print(\"f-statistics is\", f_stats)\n",
    "print(\"p-value is\", p_value)\n",
    "#interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"reject the null hypothesis\")\n",
    "else:\n",
    "    print(\"fail to reject null hypothesis\")\n",
    "\n",
    "#visualization\n",
    "sns.boxplot(data=[group1, group2, group3], color= 'skyblue')\n",
    "plt.title(\"boxplot of groups\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#23- Simulate random data from a normal distribution, then perform hypothesis testing to evaluate the means\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "group1= np.random.normal(loc= 10, scale= 5, size= 20)\n",
    "group2= np.random.normal(loc= 15, scale= 5, size= 20)\n",
    "\n",
    "t_stat, p_value= stats.ttest_ind(group1, group2)\n",
    "print(\"t-test is \", t_stat)\n",
    "print(\"p_value is\", p_value)\n",
    "\n",
    "#interpretation \n",
    "alpha= 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"reject the null hypothesis\")\n",
    "else:\n",
    "    print(\"fail to rejct the null hypothesis\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#24- Perform a hypothesis test for population variance using a Chi-square distribution and interpret the results\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "sample_data= np.random.normal(loc= 10, scale= 5, size= 20)\n",
    "data_size= len(sample_data)\n",
    "sample_variance= np.var(sample_data, ddof=1)\n",
    "\n",
    "sigma_0_squared= 25\n",
    "chi_square_stat= (data_size-1) * sample_variance / sigma_0_squared\n",
    "\n",
    "alpha= 0.05\n",
    "degree_of_freedom = data_size-1\n",
    "lower_critical_value= stats.chi2.ppf(alpha / 2, degree_of_freedom)\n",
    "upper_critical_value= stats.chi2.ppf(1-alpha / 2, degree_of_freedom)\n",
    "\n",
    "print(\"variance of sample is\", sample_variance)\n",
    "print(\"chi-square stats is\", chi_square_stat)\n",
    "print(\"critical values are\", lower_critical_value, upper_critical_value)\n",
    "\n",
    "\n",
    "#interpretation \n",
    "if chi_square_stat < lower_critical_value or chi_square_stat > upper_critical_value:\n",
    "    print(\"reject the null hypothesis\")\n",
    "else:\n",
    "    print(\"fail to reject the null hypothesis\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#25- Write a Python script to perform a Z-test for comparing proportions between two datasets or groups\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "successes_group1= 40\n",
    "size1= 90\n",
    "\n",
    "successes_group2= 50\n",
    "size2= 100\n",
    "\n",
    "#calculate sample propotions\n",
    "p1= successes_group1/ size1\n",
    "p2= successes_group2/ size2\n",
    "\n",
    "#calculate pooled propotions\n",
    "p = (successes_group1 + successes_group2) / (size1 + size2)\n",
    "z = (p1 - p2) / np.sqrt(p * (1 - p) * (1/size1 + 1/size2))\n",
    "p_value = 2 * (1 - norm.cdf(abs(z)))\n",
    "\n",
    "print(\"z-statistics is\", z)\n",
    "print(\"p_value\", p_value)\n",
    "\n",
    "#interpretation\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"reject the null hypothesis\")\n",
    "else:\n",
    "    print(\"fail to reject the null hypothesis\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#26- Implement an F-test for comparing the variances of two datasets, then interpret and visualize the results\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "#datasets\n",
    "ds1= np.array([2,4,6,8,10])\n",
    "size1= len(ds1)\n",
    "ds2= np.array([1,3,5,7,9])\n",
    "size2= len(ds2)\n",
    "\n",
    "ds1_var= np.var(ds1, ddof=1)\n",
    "ds2_var= np.var(ds2, ddof=1)\n",
    "\n",
    "f_statistics= ds1_var / ds2_var if ds1_var > ds2_var else ds2_var/ds1_var\n",
    "\n",
    "dof_freedom1= size1-1\n",
    "dof_freedom2= size2-1\n",
    "\n",
    "alpha = 0.05\n",
    "f_critical= stats.f.ppf(1- alpha, dof_freedom1, dof_freedom2)\n",
    "\n",
    "print(\"variance of dataset1 is\", ds1_var)\n",
    "print(\"variance of dataset2 is\", ds2_var)\n",
    "print(\"f-statistics is\", f_statistics)\n",
    "print(\"f-critical is\", f_critical)\n",
    "\n",
    "#interpretation \n",
    "if f_statistics > f_critical:\n",
    "    print(\"reject the null hypothesis\")\n",
    "else:\n",
    "    print(\"fail to reject the null hypothesis\")\n",
    "\n",
    "#visualization \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot([ds1, ds2],  labels= [\"ds 1\", \"ds 2\"])\n",
    "plt.title(\"boxplot of dataset 1 and dataset 2\")\n",
    "\n",
    "#histogram\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(ds1, alpha=0.7, label=\"Group 1\", bins=5)\n",
    "plt.hist(ds2, alpha=0.7, label=\"Group 2\", bins=5)\n",
    "plt.legend()\n",
    "plt.title(\"Histograms of Group 1 and Group 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#27- Perform a Chi-square test for goodness of fit with simulated data and analyze the results\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "observed = [2,4,6,8]\n",
    "expected=  [2,4,6,8]\n",
    "\n",
    "chi_square, p_value= stats.chisquare(observed, expected)\n",
    "print(\"square of chi is \", chi_square)\n",
    "print(\"value of p is\", p_value)\n",
    "\n",
    "#interpretation\n",
    "alpha = 0.05 \n",
    "if p_value < alpha:\n",
    "    print(\"reject the null hypothesis\")\n",
    "else:\n",
    "    print(\"fail to reject the null hypothesis\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
